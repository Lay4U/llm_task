{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc24db467b6737e",
   "metadata": {},
   "source": [
    "트랜스포머 아키텍처 = 언어를 이해하는 인코더 + 언어를 생성하는 디코더\n",
    "RNN -> attention is all new need -> 높은 성능과 빠른 학습속도\n",
    "순차적으로 하나씩 입력을 받는 RNN은 학습 속도가 느리고, 입력이 길어지면 먼저 입력한 토큰의 정보가 희석되면서 성능이 떨어진다. 또한 성능을 높이기 위해 층을 깊이 쌓으면 그레디언트 소실이나 증폭(gradient vanishing, exploding)이 발생하며 학습이 불안전해진다.\n",
    "셀프 어텐션: 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현을 조정하는 역할\n",
    "    확장성: 더 깊은 모들을 만들어도 학습이 잘 되고 동일한 블록을 반복해 사용하기 때문에 확장이 용이하다.\n",
    "    효율성: 학습할 때 병렬 연산이 가능하기 때문에 학습 시간이 단축된다.\n",
    "    더 긴 입력처리: 입력이 길어져도 성능이 거의 떨어지지 않는다.\n",
    "입력을 임베딩 층을 통해 숫자 집합인 임베딩으로 변환하고 위치 인코딩(positional encoding) 층에서 문장의 위치 정보를 더한다. 인코더에서는 층 정규화(layer normalization), 멀티 헤드 어텐ㅅ녀(multi head attention), 피드 포워드(feed forward)층을 거치며 문장을 이해하고 디코더로 전달한다. 디코더에서는 인코더에서와 유사하게 층 정규화, 멀티 헤드 어텐션 연산을 수행하면서 크로스 어텐션 연산을 통해 인코더가 전달한 데이터를 출력화 함께 종합해서 피드 포워드 층을 거쳐 결과를 생성한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd95ee84f7b07dd",
   "metadata": {},
   "source": [
    "텍스트를 모델에 입력할 수 있는 숫자형 데이터인 임베딩으로 변화하기 위해서는 세가지 과정을 거쳐야 한다. \n",
    "    1. 텍스트를 적절한 단위로 달라 숫자형 아이디를 부여하는 토큰화\n",
    "    2. 토큰 아이디를 토큰 임베딩 층을 통해 여러 숫자 집한인 토큰 임베딩으로 변환\n",
    "    3. 위치 인코딩 층을 통해 토큰의 위치 정보를 담고 있는 위치 임베딩을 추가해 최정적으로 모델에 입력할 임베딩을 만든다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5694776112bfd",
   "metadata": {},
   "source": [
    "토큰화: 텍스트를 적절한 단위로 나누고 숫자 아이디를 부여하는 것.\n",
    "\n",
    "큰 단위를 기준으로 토큰화 할수록 텍스트의 의미가 잘 유지된다는 장점이 있지만, 사전의 크기가 커진다는 단점이 있다.\n",
    "이전에 본 적 없는 새로운 단어는 사전에 없기 때문에 처리하지 못하는 OOV(Out Of Vocabulary) 문제가 자주 발생한다.\n",
    "작은 단위로 토큰화 하는 경우 사전의 크기가 작고 OOV 문제를 줄일 수 있지만 텍스트의 의미가 유지되지 않는다는 단점이 있다.\n",
    "두 방법 모두 장단점이 뚜렷하기 때문에 최근에는 데이터에 등장하는 빈도에 따라 토큰화 단위를 결정하는 서브워드(subword) 토큰화 방식을 사용한다.\n",
    "서브워드 토큰화 방식에서는 자주 나오는 단어는 단어 단위 그대로 유지하고 가끔 나오는 단어는 더 작은 단위로 나눠 텍스트의 의미를 최대한 유지하면서 사전의 크기는 작고 효율적으로 유지할 수 있다.\n",
    "     ex) '안녕', '대한민국' 같이 자주 나오는 단어는 그대로 단어 형태를 유지, 특수문자, 이모지 등은 작게 나눠 사전 크기가 커지지 않도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57a39402c94daf",
   "metadata": {},
   "source": [
    "sadfafa"
   ]
  },
  {
   "cell_type": "code",
   "id": "85d884e9f801479c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:58.402463Z",
     "start_time": "2024-10-12T23:21:56.308992Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:58.418775Z",
     "start_time": "2024-10-12T23:21:58.413796Z"
    }
   },
   "source": [
    "input_text = \"나는 최근 파리 여행을 다녀왔다.\"\n",
    "input_text_list = input_text.split()\n",
    "print(\"input_text_list: \", input_text_list)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text_list:  ['나는', '최근', '파리', '여행을', '다녀왔다.']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "664473fc6f6471c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:58.736460Z",
     "start_time": "2024-10-12T23:21:58.732381Z"
    }
   },
   "source": [
    "str2idx = {word:idx for idx, word in enumerate(input_text_list)}\n",
    "idx2str = {idx:word for idx, word in enumerate(input_text_list)}\n",
    "print(\"str2idx: \", str2idx)\n",
    "print(\"idx2str: \", idx2str)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "str2idx:  {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다.': 4}\n",
      "idx2str:  {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다.'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "63fc673301de431f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:58.882586Z",
     "start_time": "2024-10-12T23:21:58.879499Z"
    }
   },
   "source": [
    "input_ids = [str2idx[word] for word in input_text_list]\n",
    "print(\"input_ids: \", input_ids)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "9b5c8c778d1d2d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.015339Z",
     "start_time": "2024-10-12T23:21:59.011493Z"
    }
   },
   "source": [
    "print(input_text_list)\n",
    "print(str2idx)\n",
    "print(idx2str)\n",
    "print(input_ids)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는', '최근', '파리', '여행을', '다녀왔다.']\n",
      "{'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다.': 4}\n",
      "{0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다.'}\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "b0f27ada0103cd92",
   "metadata": {},
   "source": [
    "모델이 텍스트 데이터를 처리하기 위해서는 입력으로 들어오는 토큰과 토큰 사이의 관계를 계산할 수 있어야 한다. 토큰과 토큰 사이의 관계를 계산하기 위해서는 토큰의 의미를 숫자로 나타낼 수 있어야 하는데 토큰 아이디는 하나의 숫자일 뿐이므로 토큰의 의미를 담을 수 없다. 의미를 담기 위해서는 최소 2개 이상의 숫자 집합인 벡터여야 한다. nn.Embedding 클래스를 사용하면 토큰 아이디를 토큰 임베딩으로 변환할 수 있다. 이 코드에서는 nn.Embedding 클래스에 사전 크기가 len(str2idx)이고 embedding_dim차원의 임베딩을 생성하는 임베딩 층인 embed_layer를 만들고 입력 토큰을 임베딩 층을 통해 임베딩으로 변환한다. embedding_dim을 16으로 설정해 토큰 하나를 16차원의 벡터로 변환한다. 출력 결과를 보면 1개의 문장이고, 5개의 토큰이 있고, 16차원의 임베딩이 생성됐음을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "id": "9255f9d065ae4a30",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.109653Z",
     "start_time": "2024-10-12T23:21:59.104756Z"
    }
   },
   "source": [
    "embedding_dim = 16\n",
    "embed_layer = torch.nn.Embedding(len(str2idx), embedding_dim)\n",
    "\n",
    "input_embeddings = embed_layer(torch.tensor(input_ids))\n",
    "input_embeddings = input_embeddings.unsqueeze(0)\n",
    "print(input_embeddings.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "d15a0ca9-d8b9-42f7-8290-75fd62a078ca",
   "metadata": {},
   "source": [
    "위의 임베딩 층은 토큰의 의미를 담아 벡터로 변환하지 않는다. 지금의 임베딩 층은 그저 입력 토큰 아이디를 16차원의 임의의 숫자 집합으로 바꿔줄 뿐이다. 임베딩 층이 단어의 의미를 담기 위해서는 딥러닝 모델이 학습 모델로 훈련되어야 한다.\n",
    "모델이 특정 작업을 잘 수행하도록 학습하는 과정에서 데이터의 의미를 잘 담은 임베딩을 만드는 방법도 함께 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef2f8b-ee73-4c80-8020-0aef1c99f0f3",
   "metadata": {},
   "source": [
    "absolute position encoding vs relative position encoding\n",
    "RNN과 트랜스포머의 가장 큰 차이점은 입력을 순차적으로 처리하는지 여부다. RNN은 입력을 순차적으로 처리하는데, 그렇기 때문에 자연스럽게 입력 데이터의 순서 정보가 고려된다. 트랜스포머는 순차적인 처리 방식을 버리고 모든 입력을 동시에 처리하는데, 그 과정에서 순서 정보가 사라지게 된다. 하지만 텍스트에서 순서는 매우 중요한 정보이기 때문에 추가해 줘야 하는데, 그 역할을 위치 인코딩이 담당한다.\n",
    "Attention is ALl you need 논문에서는 사인과 코사인을 활용한 수식을 통해 위치에 대한 정보를 입력했다. 하지만 그 이후에는 위치 인코딩도 위치에 따른 임베딩 층을 추가해 학습 데이터를 통해 학습하는 방식을 많이 활용하고 있다. 수식을 통해 위치 정보를 추가하는 방식이나 임베딩으로 위치 정보를 학습하는 방식 모두 결국 모델로 추론을 수행하는 시점에서는 입력 토큰의 위치에 따라 고정된 임베딩을 더해주기 때문에, 이를 절대적 위치 인코딩이라고 부른다.\n",
    "\n",
    "절대적 위치 인코딩 방식은 간단하게 구현할 수 있다는 장점이 있지만 토큰과 토큰 사이의 상대적인 위치 정보는 활용하지 못하고, 학습 데이터에서 보기 어려웠떤 긴 텍스트를 추론하는 경우에는 성능이 떨어진다는 문제가 있어 최근에는 상대적 위치 인코딩 방식도 많이 활용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e545a-47d8-4081-bbc3-3f6b056e124a",
   "metadata": {},
   "source": [
    "절대적 위치 인코딩 중 위치 정보를 학습하는 방식을 코드로는 아래와 같이 새로운 임베딩 층을 하나 추가하고 위치 인덱스(psotion_ids)에 따라 임베딩을 더하도록 구현할 수 있다. 최대 토큰수(max_position)을 12로 설정하여 위치 인코딩을 생성하는 위치 임베딩 층(position_embed_lay)을 정의한다. 위치 아이디(position_ids)에는 0부터 입력 토큰의 수까지 1씩 증가하도록 데이터를 생성한다. position_ids를 위치 임베딩 층에 입력해 위치 인코딩(position_encodings)을 생성하고 토큰 임베딩(token_embedings)에 위치 인코딩을 더해 모델에 입력할 최종 입력 임베딩(input_embeddings)을 준비한다."
   ]
  },
  {
   "cell_type": "code",
   "id": "770054e37970965b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.160534Z",
     "start_time": "2024-10-12T23:21:59.155843Z"
    }
   },
   "source": [
    "embedding_dim = 16\n",
    "max_position = 12\n",
    "embed_layer = torch.nn.Embedding(len(str2idx), embedding_dim)\n",
    "position_embed_layer = torch.nn.Embedding(max_position, embedding_dim)\n",
    "\n",
    "position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
    "position_encodings = position_embed_layer(position_ids)\n",
    "token_embeddings = embed_layer(torch.tensor(input_ids))\n",
    "token_embeddings = token_embeddings + position_encodings\n",
    "\n",
    "print(position_ids)\n",
    "print(position_encodings.shape)\n",
    "print(token_embeddings.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4]])\n",
      "torch.Size([1, 5, 16])\n",
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "35a7e6d7-7bce-4741-907a-6d4ace874ea1",
   "metadata": {},
   "source": [
    "\"나는 최근 파리 여행을 다녀왔따\"라는 문장을 단어 단위로 토큰화를 수행하고 각 토큰에 토큰 아이디와 위치 아이디를 부여했다. \n",
    "실제 토큰 아이디와 위치 아이디 모두 [0,1,2,3,4]의 값을 갖는데, 값은 동일하지만 토큰 아이디는 사전에 저장된 토큰의 고유한 아이디를 의믜하고 위치 아이디는 토큰의 위치를 의미한다. 예시 데이터이기에 같게 표현된것이고 일반적으로는 같지 않다. 하나의 숫자로된 숫자 아이디는 데이터의 의미를 담을 수 없기 때문에 의미를 담을 수 있또록 토큰 아이디와 위치 아이디를 토큰 임베딩층과 위치 인코딩 층을 통해 토큰 임베딩과 위치 임베딩으로 변환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105719ff-aef6-4318-b230-980e61422366",
   "metadata": {},
   "source": [
    "트랜스포머 아키텍처의 핵심은 논무의 제목에서도 알 수 있듯이 '어텐션'이다. 어텐션의 사전적 의미는 '주의'라고 번역할 수 있는데, 텍스트를 처리하는 관점에서는 입력한 텍스트에서 어떤 단어가 서로 관련되는지 '주의를 기울여' 파악한다는 의미로 이해할 수 있다.\n",
    "\n",
    "트랜스포머 아키텍처를 개발한 연구진은 정보 분야에서 쿼리, 키, 값이라는 개념을 도입했다. 쿼리, 키, 값에 대한 각각의 가중치를 weight_q, weight_k, weight_v로 생성하고 입력으로 준비한 input_embedding을 선형 층에 통과시켜 쿼리, 키, 값을 생성한다.\n",
    "\n",
    "단계별로 먼저 쿼리와 키를 곱한다. 이때 분산이 커지는 것을 방지하기 위해 임베딩 차원 수의 제곱근으로 나눈다. 다음으로 쿼리와 키를 곱해 계산한 스코어를 합이 1이 되도록 소프트맥스를 취해 가중치로 바꾼다. 마지막으로 가중치와 값을 곱해 입력과 동일한 형태를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "id": "e88709e8fe353c96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.243947Z",
     "start_time": "2024-10-12T23:21:59.238872Z"
    }
   },
   "source": [
    "head_dim = 16\n",
    "\n",
    "weight_q = torch.nn.Linear(embedding_dim, head_dim)\n",
    "weight_k = torch.nn.Linear(embedding_dim, head_dim)\n",
    "weight_v = torch.nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "querys = weight_q(input_embeddings)\n",
    "keys = weight_k(input_embeddings)\n",
    "values = weight_v(input_embeddings)\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "f1a8f45b177a532a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.264522Z",
     "start_time": "2024-10-12T23:21:59.261227Z"
    }
   },
   "source": [
    "from math import sqrt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_attention(querys, keys, values, is_causal=False):\n",
    "    dim_k = querys.size(-1)\n",
    "    scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return weights @ values"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "c4acb62c0f3c3656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.284015Z",
     "start_time": "2024-10-12T23:21:59.277005Z"
    }
   },
   "source": [
    "print(input_embeddings.shape)\n",
    "after_attention_embeddings = compute_attention(querys, keys, values)\n",
    "\n",
    "print(after_attention_embeddings.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16])\n",
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "84ec4f96958b4ba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.348933Z",
     "start_time": "2024-10-12T23:21:59.344320Z"
    }
   },
   "source": [
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, token_embed_dim, head_dim, is_causal=False):\n",
    "        super().__init__()\n",
    "        self.is_causal = is_causal\n",
    "        self.weight_q = nn.Linear(token_embed_dim, head_dim)\n",
    "        self.weight_k = nn.Linear(token_embed_dim, head_dim)\n",
    "        self.weight_v = nn.Linear(token_embed_dim, head_dim)\n",
    "        \n",
    "    def forward(self, querys, keys, values):\n",
    "        outputs = compute_attention(\n",
    "            self.weight_q(querys),\n",
    "            self.weight_k(keys),\n",
    "            self.weight_v(values),\n",
    "            is_causal = self.is_causal\n",
    "        )\n",
    "        return outputs\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "3549aa4fec90c9b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.371635Z",
     "start_time": "2024-10-12T23:21:59.367192Z"
    }
   },
   "source": [
    "attention_head = AttentionHead(embedding_dim, embedding_dim)\n",
    "after_attention_embeddings = attention_head(querys, keys, values)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "1e3242c3-d515-4210-be64-f8ee9ddb190b",
   "metadata": {},
   "source": [
    "트랜스포머 아키텍처를 고안한 논문 저자들은 한번에 하나의 어텐션 연산만 수행하는게 아니라 여러 어텐션 연산을 동시에 적용하면 성능을 더 높일 수 있다는 사실을 발견했다. 이를 멀티 헤드 어텐션이라고 한다. 직관적으로 이해하자면, 토큰 사이의 관계를 한가지 측면에서 이해하는 것보다 여러 측면을 동시에 고려할 때 언어나 문장에 대한 이해도가 높아질 것이다.\n",
    "\n",
    "AttentionHead와 대부분의 코드가 동일한데 헫드의 수만큼 연산을 수행하기 위해 쿼리, 키, 값을 n_head개로 쪼개고 각각의 어텐션을 계산한 다음 입력과 같은 형태로 다시 변환한다. 마지막으로 선형층을 통과시키고 최종 결과를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "id": "cefeb4e429261c1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.395402Z",
     "start_time": "2024-10-12T23:21:59.388947Z"
    }
   },
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, token_embed_dim, d_model, n_head, is_causal=False):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.is_causal = is_causal\n",
    "        self.weight_q = nn.Linear(token_embed_dim, d_model)\n",
    "        self.weight_k = nn.Linear(token_embed_dim, d_model)\n",
    "        self.weight_v = nn.Linear(token_embed_dim, d_model)\n",
    "        self.concat_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, querys, keys, values):\n",
    "        B, T, C = querys.size()\n",
    "        querys = self.weight_q(querys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        keys = self.weight_k(keys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        values = self.weight_v(values).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        attention = compute_attention(querys, keys, values, self.is_causal)\n",
    "        output = attention.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        output = self.concat_linear(output)\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "2ce299dbab6baae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.421617Z",
     "start_time": "2024-10-12T23:21:59.411238Z"
    }
   },
   "source": [
    "n_head = 4\n",
    "mh_attention = MultiheadAttention(embedding_dim, embedding_dim, n_head)\n",
    "after_attention_embeddings = mh_attention(input_embeddings, input_embeddings, input_embeddings)\n",
    "after_attention_embeddings.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "a5329a99-f03e-4f97-b0ca-b686b40bf598",
   "metadata": {},
   "source": [
    "데이터를 정규화하여 모든 입력 변수가 비슷한 범위의 분포를 갖도록 조정하면 모델은 각 입력 변수의 중요성을 적절히 반영하여 좀 더 정확한 예측을 할 수 있게 된다.\n",
    "\n",
    "자연어 처리에서는 입력으로 들어가는 문장의 길이가 다양한데, 배치 정규화를 사용할 경우 정규화에 포함되는 데이터의 수가 제각각이라 정규화 효과를 보장하기 어렵기 때문에 배치 정규화를 사용하지 않는다.\n",
    "\n",
    "층 정규화는 이런 단점을 보완할 수 있도록 각 토큰 임베딩의 평균과 표준편차를 구해 정규화를 수행한다. 문장별로 실제 데이터의 수가 다르더라도 각각의 토큰 임베딩별로 정규화를 수행하기 때문에 정규화 효과에 차이가 없다.\n",
    "\n",
    "On Layer Normalizzation in the Transformer Architecture 논문에서 먼저 층 정규화를 적용하고 어텐션과 피드 포워드 층을 통화했을 때 학습이 더 안정적이라는 사실이 확인되어 post norm 대신 pre norm이 주로 활용된다."
   ]
  },
  {
   "cell_type": "code",
   "id": "ce7c4e2e061404a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.480027Z",
     "start_time": "2024-10-12T23:21:59.474533Z"
    }
   },
   "source": [
    "norm = nn.LayerNorm(embedding_dim)\n",
    "norm_x = norm(input_embeddings)\n",
    "norm_x.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "7463bb16f37aafbd",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.560245Z",
     "start_time": "2024-10-12T23:21:59.553824Z"
    }
   },
   "source": [
    "norm_x.mean(dim=-1).data, norm_x.std(dim=-1).data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.4901e-08, -2.6077e-08, -2.6077e-08, -1.1176e-08,  1.4901e-08]]),\n",
       " tensor([[1.0328, 1.0328, 1.0328, 1.0328, 1.0328]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "8d50906f-ad56-4ae1-a19b-419e57cd101d",
   "metadata": {},
   "source": [
    "피드 포워드 층은 데이터의 특징을 학습하는 완전 연결 층을 말한다. 멀티 헤드 어텐션이 단어 사의의 관계를 파악하는 역할이라면 피드 포워드 층은 입력 텍스트 전체를 이해하는 역할을 담당한다."
   ]
  },
  {
   "cell_type": "code",
   "id": "dbeb97b3308cd1f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.640561Z",
     "start_time": "2024-10-12T23:21:59.635695Z"
    }
   },
   "source": [
    " class PreLayerNormFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forard(self, src):\n",
    "        x = self.norm(src)\n",
    "        x = x + self.linear2(self.dropout1(self.activation(self.linear1(x))))\n",
    "        x = self.dropout2(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "de28f4b1-6c64-41c3-929d-1df3c0ec512e",
   "metadata": {},
   "source": [
    "인코더 구조에 따라 입력인 src를 self.norm을 통해 층 정규화를 취하고 멀티 헤드 어텐션 클래스를 인스턴스화한 self.attn을 통해 멀티 헤드 어텐션 연산을 수행한 후 잔차 연결을 위해 어텐션 결과에 드롭아웃을 위한 self.dropout1과 입력을 더해준다. 마지막으로 self.feed_forward를 통해 피드 포원드 연산을 취한다."
   ]
  },
  {
   "cell_type": "code",
   "id": "46f2bc7437be8d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.664391Z",
     "start_time": "2024-10-12T23:21:59.659976Z"
    }
   },
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init(self, d_model, dim_feedforward, dropout):\n",
    "        super().__init__(self)\n",
    "        self.attn = MultiheadAttention(d_model, d_model, n_head)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout)\n",
    "        \n",
    "    def forard(self, src):\n",
    "        norm_x = self.norm1(src)\n",
    "        attn_output = self.attn(norm_x, norm_x, norm_x)\n",
    "        x = src + self.dropout1(attn_output)\n",
    "        \n",
    "        x = self.feed_forward(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "c05c8604-0c3c-4e61-9ce2-9d47ba2a57cd",
   "metadata": {},
   "source": [
    "인코더 층을 N_e번 반복되도록 코드로 구현한다. TransformerEncoder 클래스에서는 인자로 전달받은 encoder_layer를 get_clones 함수를 통해 num_layers번 반복하여 nn.ModuleList에 넣고 forward 메서드에서 for 문을 통해 순회하면서 인코더 층 연산을 반복 수행하도록 만든다."
   ]
  },
  {
   "cell_type": "code",
   "id": "ec6b3e5f8203360c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.686227Z",
     "start_time": "2024-10-12T23:21:59.681962Z"
    }
   },
   "source": [
    "import copy\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        \n",
    "    def forward(self, src):\n",
    "        output = src\n",
    "        for mod in self.layers:\n",
    "            for mod in self.layers:\n",
    "                output = mod(output)\n",
    "            return output"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "디코더는 인코더와 비교할 때 두가지 부분에서 차이가 있다. 먼저, 인코더는 멀티 헤드 어텐션을 사용하지만 디코더 블록에서는 마스크 멀티 헤드 어텐션을 사용한다. 디코더는 생성을 담당하는 부분으로, 사람이 글을 쓸 때 앞 단어부터 순차적으로 작성하는 것처럼 트랜스포머 모델도 앞에서 생성한 토큰을 기반으로 다음 토큰을 생성한다. 이렇게 순차적으로 생성해야 하는 특징을 인과적(causal) 또는 자기회귀적(auto-regressive)이라고 한다.",
   "id": "1410bd1a0f7867a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "실제 텍스트를 생성할 때 디코더는 이전까지 생성한 텍스트만 확인할 수 있다. 그런데 학습할 때는 인코더와 디코더 모두 완성된 텍스트를 입력으로 받는다. 따라서 어텐션을 그대로 할용할 경우 미래 시점에 작성해야 하는 텍스트를미리 확인하게 되는 문제가 생긴다. 이를 막기 위해서는 특정 시점에서는 그 이전에 생성된 토큰까지만 확인할 수 있도록 마스크를 추가한다. \n",
    "\n",
    "is_causal이 참일 때는 torch.ones로 모두 1인 행렬에 tril 함수를 취해 가운데 행렬과 같이 대각선 아래 부분만 1로 유지되고 나머지는 음의 무한대로 변경해 마스크를 생성한다. 마스크를 어텐션 스코어 행렬에 곱하면 행렬의 대각선 아랫부분만 어텐션 스코어가 남고 위쪽은 음의 무한대가 된다. 소프트 맥스를 취하면 음의 무한대인 대각선 윗부분은 가중치가 0이 된다."
   ],
   "id": "def01837b98ae340"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.704971Z",
     "start_time": "2024-10-12T23:21:59.700307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_attention(querys, keys, values, is_causal=False):\n",
    "    dim_k = querys.size(-1)\n",
    "    scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n",
    "    if is_causal:\n",
    "        query_length = querys.size(2)\n",
    "        key_length = keys.size(2)\n",
    "        temp_mask = torch.ones(query_length, key_length, dtype=torch.bool).tril(diagonal=0)\n",
    "        scores = scores.masked_fill(temp_mask == False, float(\"-inf\"))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return weights @ values\n",
    "        "
   ],
   "id": "4352b818492765b6",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "인코더의 결과를 디코더가 활용하는 크로스 어텐션 연산을 해야 한다. 이때 쿼리는 디코더의 잠재 상태를 사용하고 키와 값은 인코더의 결과를 사용한다\n",
    "\n",
    "인코더의 결과를 forward 메서드에 encoder_output이라는 이름의 인자로 넣을 수 있도록 했는데 self.multihead_attn을 통해 크로스어텐션 연산을 수행한다.\n",
    "\n",
    "디코더는 인코더와 마찬가지로 디코더 층을 여러 번 쌓아 만든다. 디코더 층을 N번 반복하여 nn.ModuleList를 활용해 순회하도록 한다."
   ],
   "id": "4c38ff746277a28d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.724223Z",
     "start_time": "2024-10-12T23:21:59.719035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, d_model, n_head)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, d_model, n_head)\n",
    "        self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, tgt, encoder_output, is_causal=True):\n",
    "        x = self.norm1(tgt)\n",
    "        x = x + self.dropout1(self.self_attn(x,x,x,is_causal=is_causal))\n",
    "        x = self.norm2(x)\n",
    "        x = x + self.dropout2(self.multihead_attn(x, encoder_output, encoder_output))\n",
    "        x = self.feed_forward(x)\n",
    "        return x"
   ],
   "id": "6a97c45e56f54edb",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.741805Z",
     "start_time": "2024-10-12T23:21:59.737854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, decoder_layer, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, tgt, src):\n",
    "        output = tgt\n",
    "        for mod in self.layers:\n",
    "            output = mod(output)\n",
    "        return output"
   ],
   "id": "6d072064542775c3",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.758847Z",
     "start_time": "2024-10-12T23:21:59.756123Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e257ab0d262c995c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T23:21:59.775530Z",
     "start_time": "2024-10-12T23:21:59.772804Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7f90e3ea66adc254",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
