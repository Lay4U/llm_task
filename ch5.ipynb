{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-14T11:43:56.891642Z",
     "start_time": "2024-10-14T11:43:54.215903Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        used_memory = torch.cuda.memory_allocated() / 1024 ** 3\n",
    "        print(f\"GPU 메모리 사용량: {used_memory: .3f} GB\")\n",
    "    else:\n",
    "        print(\"런타임 유형을 GPU로 변경하세요\")\n",
    "        \n",
    "print_gpu_utilization()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  0.000 GB\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T11:44:06.501252Z",
     "start_time": "2024-10-14T11:43:59.953132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\":0})\n",
    "        \n",
    "    print_gpu_utilization()\n",
    "    return model, tokenizer\n",
    "\n",
    "model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_id)\n",
    "print('모델 파라미터 데이터 타입: ', model.dtype)"
   ],
   "id": "6b615d26a251d877",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmr34\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab23c2e9be5b48bf99aaccbf31eb9284"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmr34\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  2.599 GB\n",
      "모델 파라미터 데이터 타입:  torch.float16\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T11:44:06.526089Z",
     "start_time": "2024-10-14T11:44:06.513585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def estimate_memory_of_gradients(model):\n",
    "    total_memory = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            total_memory += param.grad.nelement() * param.grad.element_size()\n",
    "    return total_memory\n",
    "\n",
    "def estimate_memory_of_optimizer(optimizer):\n",
    "    total_memory = 0\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if torch.is_tensor(v):\n",
    "                total_memory += v.nelement() * v.element_size()\n",
    "    return total_memory\n"
   ],
   "id": "4129594ac11338ba",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T11:44:07.603291Z",
     "start_time": "2024-10-14T11:44:07.597005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, dataset, training_args):\n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset, batch_size=training_args.per_device_train_batch_size)\n",
    "    optimizer = AdamW(model.parameters())\n",
    "    model.train()\n",
    "    gpu_utilization_printed = False\n",
    "    for step, batch in enumerate(train_dataloader, start=1):\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / training_args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if step % training_args.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            gradients_memory = estimate_memory_of_gradients(model)\n",
    "            optimizer_memory = estimate_memory_of_optimizer(optimizer)\n",
    "            if not gpu_utilization_printed:\n",
    "                print_gpu_utilization()\n",
    "                gpu_utilization_printed = True\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        print(f\"옵티마이저 상태의 메모리 사용량: {optimizer_memory / (1024 ** 3):.3f} GB\")\n",
    "        print(f\"그레디언트 메모리 사용량: {gradients_memory / (1024 ** 3):.3f} GB\")"
   ],
   "id": "82328e6b6fb39312",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T11:44:09.644853Z",
     "start_time": "2024-10-14T11:44:08.061934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "def make_dummy_dataset():\n",
    "    seq_len, dataset_size = 256, 64\n",
    "    dummy_data = {\n",
    "        \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "        \"labels\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    }\n",
    "    dataset = Dataset.from_dict(dummy_data)\n",
    "    dataset.set_format(\"pt\")\n",
    "    return dataset"
   ],
   "id": "810f119e07cd6754",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T11:44:11.201123Z",
     "start_time": "2024-10-14T11:44:11.197763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    if 'model' in globals():\n",
    "        del globals()['model']\n",
    "    if 'datset' in globals():\n",
    "        del globals()['dataset']\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n"
   ],
   "id": "31a5a846f3830e09",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T11:44:17.549610Z",
     "start_time": "2024-10-14T11:44:11.736281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "def gpu_memory_experiment(batch_size,\n",
    "                          gradient_accumulation_steps=1,\n",
    "                          gradient_checkpointing=False,\n",
    "                          model_id=\"EleutherAI/polyglot-ko-1.3b\",\n",
    "                          peft=None):\n",
    "    print(f\"배치 크기: {batch_size}\")\n",
    "    model, tokenizer = load_model_and_tokenizer(model_id, peft=peft)\n",
    "    if gradient_checkpointing == True or peft == 'qlora':\n",
    "        model.config.use_cache = False\n",
    "    \n",
    "    dataset = make_dummy_dataset()\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size =batch_size,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        gradient_checkpointing=gradient_checkpointing,\n",
    "        output_dir=\"./result\",\n",
    "        num_train_epochs=1\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        train_model(model, dataset, training_args)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(e)\n",
    "        else:\n",
    "            raise e\n",
    "    finally:\n",
    "        del model, dataset\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print_gpu_utilization()"
   ],
   "id": "2f5986d42a500ffe",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T11:42:09.303922Z",
     "start_time": "2024-10-14T11:42:09.053651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cleanup()\n",
    "# print_gpu_utilization()\n",
    "# \n",
    "# for batch_size in [4, 8, 16]:\n",
    "#     gpu_memory_experiment(batch_size)\n",
    "#     \n",
    "# torch.cuda.empty_cache()"
   ],
   "id": "7b4f7079d30f6c6f",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m cleanup()\n\u001B[0;32m      2\u001B[0m print_gpu_utilization()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_size \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m16\u001B[39m]:\n",
      "Cell \u001B[1;32mIn[7], line 9\u001B[0m, in \u001B[0;36mcleanup\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mglobals\u001B[39m()[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataset\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      8\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[1;32m----> 9\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\cuda\\memory.py:170\u001B[0m, in \u001B[0;36mempty_cache\u001B[1;34m()\u001B[0m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001B[39;00m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001B[39;00m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;124;03m`nvidia-smi`.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;124;03m    more details about GPU memory management.\u001B[39;00m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_initialized():\n\u001B[1;32m--> 170\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_cuda_emptyCache()\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "그레디언트 누적을 사용하면 딥러닝 모델을 학습시킬 때 각 배치마다 모델을 업데이트 하지 않고 여러 배치의 학습 데이터를 연산한 후 모델을 업데이트해 마치 더 큰 배치 크기를 사용하는 것 같은 효과를 낼 수 있다.\n",
    "\n",
    "그레디언트 체크포인팅은 순전파의 계산 결과를 모두 저장하지 않고 일부만 저장해 학습 중 GPU 메모리의 사용량을 줄이는 학습 방법이다. 두 방법 모두 모델 학습 시에 배치 크기를 키워 모델의 학습을 더 빠르고 안정적으로 만들어준다.\n"
   ],
   "id": "c9269f05515ece1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T11:44:43.773385Z",
     "start_time": "2024-10-14T11:44:38.254227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()\n",
    "gpu_memory_experiment(batch_size=4, gradient_accumulation_steps=4)\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "db15e7cd99051b4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  0.000 GB\n",
      "배치 크기: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmr34\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d83f69c9cd44eeca5fc86249f9ff77a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  2.599 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmr34\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  5.623 GB\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'optimizer_memory' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnboundLocalError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m cleanup()\n\u001B[0;32m      2\u001B[0m print_gpu_utilization()\n\u001B[1;32m----> 3\u001B[0m gpu_memory_experiment(batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, gradient_accumulation_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\n\u001B[0;32m      4\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n",
      "Cell \u001B[1;32mIn[7], line 24\u001B[0m, in \u001B[0;36mgpu_memory_experiment\u001B[1;34m(batch_size, gradient_accumulation_steps, gradient_checkpointing, model_id, peft)\u001B[0m\n\u001B[0;32m     15\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m     16\u001B[0m     per_device_train_batch_size \u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m     17\u001B[0m     gradient_accumulation_steps \u001B[38;5;241m=\u001B[39m gradient_accumulation_steps,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     20\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     21\u001B[0m )\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 24\u001B[0m     train_model(model, dataset, training_args)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCUDA out of memory\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e):\n",
      "Cell \u001B[1;32mIn[4], line 26\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, dataset, training_args)\u001B[0m\n\u001B[0;32m     23\u001B[0m         gpu_utilization_printed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     24\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m옵티마이저 상태의 메모리 사용량: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moptimizer_memory\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m(\u001B[38;5;241m1024\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m GB\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m그레디언트 메모리 사용량: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgradients_memory\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39m(\u001B[38;5;241m1024\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m3\u001B[39m)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m GB\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mUnboundLocalError\u001B[0m: cannot access local variable 'optimizer_memory' where it is not associated with a value"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T11:47:03.043294Z",
     "start_time": "2024-10-14T11:45:12.671676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()\n",
    "gpu_memory_experiment(batch_size=4, gradient_checkpointing=True)\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "919f93b496b5a297",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  5.623 GB\n",
      "배치 크기: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3c99d848a5d43f997fe612232d2513e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  8.221 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmr34\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  15.724 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레디언트 메모리 사용량: 2.481 GB\n",
      "GPU 메모리 사용량:  5.623 GB\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ZeRO를 사용하면 SLI 환경에서 60분의1 수준으로 메모리를 줄일 수 있다.Transformer에서 Zero를 사용하는 방법은 Accelerate의 deepspeed guide를 참고한다.",
   "id": "70a8d3c9dab14fe2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T21:54:20.043201Z",
     "start_time": "2024-10-14T21:54:20.019839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\":0})\n",
    "        \n",
    "    elif peft == 'lora':\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\":0})\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"query_key_value\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        \n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "        \n",
    "    print_gpu_utilization()\n",
    "    return model, tokenizer"
   ],
   "id": "3eb70929e90cd13e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T21:57:48.334350Z",
     "start_time": "2024-10-14T21:56:07.585848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()\n",
    "\n",
    "gpu_memory_experiment(batch_size=16, peft='lora')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "3ef9a5cfcb7935d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  5.623 GB\n",
      "배치 크기: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8cc08deddb64f3e9639626adc6499f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,333,383,168 || trainable%: 0.11796039111242178\n",
      "GPU 메모리 사용량:  8.224 GB\n",
      "GPU 메모리 사용량:  10.339 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.006 GB\n",
      "그레디언트 메모리 사용량: 0.003 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.006 GB\n",
      "그레디언트 메모리 사용량: 0.003 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.006 GB\n",
      "그레디언트 메모리 사용량: 0.003 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.006 GB\n",
      "그레디언트 메모리 사용량: 0.003 GB\n",
      "GPU 메모리 사용량:  5.623 GB\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T10:25:02.293952Z",
     "start_time": "2024-10-15T10:25:02.283848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def quantile_normal(p):\n",
    "    return norm.ppf(p)\n",
    "\n",
    "print(quantile_normal(0.5))\n",
    "print(quantile_normal(0.6))"
   ],
   "id": "815b7aff05cad44c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.2533471031357997\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T11:28:55.458317Z",
     "start_time": "2024-10-15T11:28:49.263056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)"
   ],
   "id": "8f67685ca3f03fb8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f1421a78e3840f4b2002b9ccf2723a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T11:37:12.511791Z",
     "start_time": "2024-10-15T11:37:12.503918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\":0})\n",
    "    elif peft == 'lora':\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map={\"\":0})\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"query_key_value\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type='CAUSAL_LM'\n",
    "        )\n",
    "        \n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "    elif peft == 'qlora':\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=['query_key_value'],\n",
    "            lora_dropout=0.05,\n",
    "            bias='none',\n",
    "            task_type='CAUSAL_LM'\n",
    "        )\n",
    "        \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "        \n",
    "        print_gpu_utilization()\n",
    "        return model, tokenizer"
   ],
   "id": "7ea3d35f43f4d171",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T11:49:51.246007Z",
     "start_time": "2024-10-15T11:37:36.392294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()\n",
    "\n",
    "gpu_memory_experiment(batch_size=16, peft='qlora')\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ],
   "id": "b898da00e8da885b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  6.552 GB\n",
      "배치 크기: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c472ced313f4132ba80cef4da42c988"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,333,383,168 || trainable%: 0.11796039111242178\n",
      "GPU 메모리 사용량:  7.719 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmr34\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  8.258 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.012 GB\n",
      "그레디언트 메모리 사용량: 0.006 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.012 GB\n",
      "그레디언트 메모리 사용량: 0.006 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.012 GB\n",
      "그레디언트 메모리 사용량: 0.006 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.012 GB\n",
      "그레디언트 메모리 사용량: 0.006 GB\n",
      "GPU 메모리 사용량:  6.552 GB\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d3831b6c1650cc69"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
